{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e98c7d2-151b-4584-8c51-3824572cf4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45008277-9f31-4781-9286-26d2ffc345ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2190e79-c9f8-4f0c-9ad8-b9d4015851f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run data_preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41851298-8f5f-4ae8-8fe9-1f6a474c1eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets_prepro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a00bc8-485c-4722-bf26-1b6a54195f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE OF EMBEDDING (c est pour avoir l idee de comment ça marche)\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "# Step 1: Tokenize and encode in one go\n",
    "encoded_sentence = tokenizer.encode('J'aime le camembert !', return_tensors='pt')\n",
    "\n",
    "# encoded_sentence is now a tensor of shape (1, sequence_length)\n",
    "\n",
    "# Step 2: Pass through CamemBERT\n",
    "outputs = camembert(encoded_sentence)\n",
    "\n",
    "# Step 3: Extract token embeddings\n",
    "embeddings = outputs.last_hidden_state  # shape: (1, seq_len, hidden_dim)\n",
    "\n",
    "\n",
    "#sentence_embedding = embeddings.mean(dim=1)  # average over tokens\n",
    "\n",
    "embeddings.shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad5cff4-62af-4ec3-8229-c22e4ed33a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_corpus = pd.read_csv('corpus_SexistContent.csv', sep='\\t', header=None, names=['tweet_id', 'label'])\n",
    "df_whole = pd.merge(df, annotated_corpus, on = 'tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b72af-4f03-4747-9641-f1e2c7c4a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertModel.from_pretrained(\"camembert-base\").to(device)\n",
    "model.eval()  # Put model in evaluation mode\n",
    "\n",
    "# Dictionary to store everything\n",
    "embeddings_dict = {}\n",
    "\n",
    "# Process each row\n",
    "for _, row in tqdm(df_whole.iterrows()):\n",
    "    sentence = row['text_clean']\n",
    "    label = row['label']\n",
    "    \n",
    "    # Tokenize and encode sentence\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=False)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}  # Move inputs to GPU\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Extract per-token embeddings and move to CPU before converting to numpy\n",
    "        token_embeddings = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "    \n",
    "    # Save to dictionary\n",
    "    embeddings_dict[sentence] = {\n",
    "        \"embeddings\": token_embeddings,  # shape: (num_tokens, 768)\n",
    "        \"label\": label\n",
    "    }\n",
    "\n",
    "# Save to .pkl\n",
    "with open(\"labeled_token_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embeddings_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fc0ce3-b310-4262-931c-eb8a84f099d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_whole.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54a1765-6636-4df1-998f-f0da07d782d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084dc08-fc4e-40df-af93-ff595da98cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    df_whole[['text_clean', 'label']],\n",
    "    test_size=0.2,\n",
    "    stratify=df_whole['label'],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2199f7-3b04-49d6-9f9b-310e01d1fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=256):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, \"text_clean\"]\n",
    "        label = self.df.loc[idx, \"label\"]\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076ef000-3384-488e-9cf2-56dc1655eae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 #as in the article\n",
    "\n",
    "train_dataset = TextDataset(train_df, tokenizer)\n",
    "test_dataset = TextDataset(test_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca10409f-f718-40d3-b75e-af4a8672df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamembertLSTMClassifier(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, lstm_layers=1):\n",
    "        super().__init__()\n",
    "        self.backbone = CamembertModel.from_pretrained(\"camembert-base\")\n",
    "        \n",
    "        # Freeze CamemBERT\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Set LSTM to be bidirectional\n",
    "        self.cnn1 = nn.\n",
    "        self.lstm1 = nn.LSTM(input_size=768, hidden_size=hidden_dim, num_layers=1, \n",
    "                             batch_first=True, bidirectional=True)  # Bidirectional LSTM\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_dim * 2, hidden_size=hidden_dim, num_layers=1, \n",
    "                             batch_first=True, bidirectional=True)  # Bidirectional LSTM\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, 1)  # We now have double the hidden size because of bidirectionality\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        sequence_output = outputs.last_hidden_state\n",
    "\n",
    "        # LSTM layers\n",
    "        lstm_out1, _ = self.lstm1(sequence_output)  # Output shape: [batch_size, seq_len, hidden_dim * 2]\n",
    "        relu_out = self.relu(lstm_out1)\n",
    "        lstm_out2, _ = self.lstm2(relu_out)  # Output shape: [batch_size, seq_len, hidden_dim * 2]\n",
    "\n",
    "        cls_token_out = lstm_out2[:, 0, :]  # Get the output of the [CLS] token\n",
    "\n",
    "        logits = self.classifier(cls_token_out)  # Final output shape: [batch_size, 1]\n",
    "        return logits.squeeze(1)  # Output: [batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a6ac8c-74f5-4bfe-805c-e312e90ba0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CamembertLSTMClassifier().to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Use sigmoid during evaluation\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f6cb3-c20f-4894-8188-e6542843d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask).squeeze()\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask).squeeze()\n",
    "            probs = torch.sigmoid(logits)\n",
    "\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_preds.extend((probs > 0.5).int().cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} — Loss: {total_loss/len(train_loader):.4f} | \"\n",
    "          f\"Accuracy: {accuracy:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16ab822-52d9-455f-b3a7-179f16623a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
