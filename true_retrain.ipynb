{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e98c7d2-151b-4584-8c51-3824572cf4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45008277-9f31-4781-9286-26d2ffc345ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2190e79-c9f8-4f0c-9ad8-b9d4015851f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=16rfiy-WrqBVBsrmE5VZk-Czk10wMAAmF\n",
      "To: /home/onyxia/work/sexism_tweets/tweets.csv\n",
      "100%|██████████| 1.39M/1.39M [00:00<00:00, 44.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "%run data_preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41851298-8f5f-4ae8-8fe9-1f6a474c1eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets_prepro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a00bc8-485c-4722-bf26-1b6a54195f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE OF EMBEDDING (c est pour avoir l idee de comment ça marche)\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "# Step 1: Tokenize and encode in one go\n",
    "encoded_sentence = tokenizer.encode('J'aime le camembert !', return_tensors='pt')\n",
    "\n",
    "# encoded_sentence is now a tensor of shape (1, sequence_length)\n",
    "\n",
    "# Step 2: Pass through CamemBERT\n",
    "outputs = camembert(encoded_sentence)\n",
    "\n",
    "# Step 3: Extract token embeddings\n",
    "embeddings = outputs.last_hidden_state  # shape: (1, seq_len, hidden_dim)\n",
    "\n",
    "\n",
    "#sentence_embedding = embeddings.mean(dim=1)  # average over tokens\n",
    "\n",
    "embeddings.shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ad5cff4-62af-4ec3-8229-c22e4ed33a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_corpus = pd.read_csv('corpus_SexistContent.csv', sep='\\t', header=None, names=['tweet_id', 'label'])\n",
    "df_whole = pd.merge(df, annotated_corpus, on = 'tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b72af-4f03-4747-9641-f1e2c7c4a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertModel.from_pretrained(\"camembert-base\").to(device)\n",
    "model.eval()  # Put model in evaluation mode\n",
    "\n",
    "# Dictionary to store everything\n",
    "embeddings_dict = {}\n",
    "\n",
    "# Process each row\n",
    "for _, row in tqdm(df_whole.iterrows()):\n",
    "    sentence = row['text_clean']\n",
    "    label = row['label']\n",
    "    \n",
    "    # Tokenize and encode sentence\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=False)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}  # Move inputs to GPU\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Extract per-token embeddings and move to CPU before converting to numpy\n",
    "        token_embeddings = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "    \n",
    "    # Save to dictionary\n",
    "    embeddings_dict[sentence] = {\n",
    "        \"embeddings\": token_embeddings,  # shape: (num_tokens, 768)\n",
    "        \"label\": label\n",
    "    }\n",
    "\n",
    "# Save to .pkl\n",
    "with open(\"labeled_token_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embeddings_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80fc0ce3-b310-4262-931c-eb8a84f099d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_whole.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b54a1765-6636-4df1-998f-f0da07d782d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>326796299179548672</td>\n",
       "      <td>@MamzelleMNa Une très humble femme! #Ironie</td>\n",
       "      <td>mamzellemna une très humble femme ironie</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>334548844731826176</td>\n",
       "      <td>BLOGUE - «Tsé, la parité homme-femme...» au se...</td>\n",
       "      <td>blogue  tsé la parité hommefemme au sein de vi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>334424362033762304</td>\n",
       "      <td>Je suis une femme matérialiste et superficiell...</td>\n",
       "      <td>je suis une femme matérialiste et superficiell...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>421708259716636672</td>\n",
       "      <td>Mise en ligne de mon article sur @PayeTaShnek ...</td>\n",
       "      <td>mise en ligne de mon article sur payetashnek p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>420942263154249728</td>\n",
       "      <td>Achat du jour : le très bon livre #PayeTaShnek...</td>\n",
       "      <td>achat du jour  le très bon livre payetashnek  ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7026</th>\n",
       "      <td>990112883215360000</td>\n",
       "      <td>#SégolèneRoyal a participé activement au décli...</td>\n",
       "      <td>ségolèneroyal a participé activement au déclin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7027</th>\n",
       "      <td>991730416158568449</td>\n",
       "      <td>Depuis l'affaire DSK, les féministes ne veulen...</td>\n",
       "      <td>depuis laffaire dsk les féministes ne veulent ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7028</th>\n",
       "      <td>991744626984980480</td>\n",
       "      <td>Analogie. C’est comme dire à une femme, vous ê...</td>\n",
       "      <td>analogie cest comme dire à une femme vous êtes...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7029</th>\n",
       "      <td>991769637506486274</td>\n",
       "      <td>Si t'as une bite a la place du coeur, t'étonne...</td>\n",
       "      <td>si tas une bite a la place du coeur tétonne pa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7030</th>\n",
       "      <td>992014962582327296</td>\n",
       "      <td>@Admir_fdlrc @cardoso_lisa @nadine__morano au ...</td>\n",
       "      <td>admir_fdlrc cardoso_lisa nadine__morano au bou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7031 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id                                               text  \\\n",
       "0     326796299179548672        @MamzelleMNa Une très humble femme! #Ironie   \n",
       "1     334548844731826176  BLOGUE - «Tsé, la parité homme-femme...» au se...   \n",
       "2     334424362033762304  Je suis une femme matérialiste et superficiell...   \n",
       "3     421708259716636672  Mise en ligne de mon article sur @PayeTaShnek ...   \n",
       "4     420942263154249728  Achat du jour : le très bon livre #PayeTaShnek...   \n",
       "...                  ...                                                ...   \n",
       "7026  990112883215360000  #SégolèneRoyal a participé activement au décli...   \n",
       "7027  991730416158568449  Depuis l'affaire DSK, les féministes ne veulen...   \n",
       "7028  991744626984980480  Analogie. C’est comme dire à une femme, vous ê...   \n",
       "7029  991769637506486274  Si t'as une bite a la place du coeur, t'étonne...   \n",
       "7030  992014962582327296  @Admir_fdlrc @cardoso_lisa @nadine__morano au ...   \n",
       "\n",
       "                                             text_clean  label  \n",
       "0              mamzellemna une très humble femme ironie      0  \n",
       "1     blogue  tsé la parité hommefemme au sein de vi...      0  \n",
       "2     je suis une femme matérialiste et superficiell...      0  \n",
       "3     mise en ligne de mon article sur payetashnek p...      0  \n",
       "4     achat du jour  le très bon livre payetashnek  ...      0  \n",
       "...                                                 ...    ...  \n",
       "7026  ségolèneroyal a participé activement au déclin...      1  \n",
       "7027  depuis laffaire dsk les féministes ne veulent ...      1  \n",
       "7028  analogie cest comme dire à une femme vous êtes...      1  \n",
       "7029  si tas une bite a la place du coeur tétonne pa...      1  \n",
       "7030  admir_fdlrc cardoso_lisa nadine__morano au bou...      1  \n",
       "\n",
       "[7031 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cb943f6-a05d-45d5-97e0-a8312ee17ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertModel.from_pretrained(\"camembert-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9084dc08-fc4e-40df-af93-ff595da98cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    df_whole[['text_clean', 'label']],\n",
    "    test_size=0.2,\n",
    "    stratify=df_whole['label'],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee2199f7-3b04-49d6-9f9b-310e01d1fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=256):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, \"text_clean\"]\n",
    "        label = self.df.loc[idx, \"label\"]\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "076ef000-3384-488e-9cf2-56dc1655eae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 #as in the article\n",
    "\n",
    "train_dataset = TextDataset(train_df, tokenizer)\n",
    "test_dataset = TextDataset(test_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca10409f-f718-40d3-b75e-af4a8672df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import CamembertModel\n",
    "\n",
    "class CamembertCNNLSTMClassifier(nn.Module):\n",
    "    def __init__(self, conv_out_dim=256, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.backbone = CamembertModel.from_pretrained(\"camembert-base\")\n",
    "        \n",
    "        # Freeze CamemBERT\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Conv1D: in_channels=768 (Camembert output dim), out_channels=conv_out_dim\n",
    "        self.conv1d = nn.Conv1d(in_channels=768, out_channels=conv_out_dim, kernel_size=3, padding=1)\n",
    "        self.relu_conv = nn.ReLU()\n",
    "\n",
    "        # Bidirectional LSTM\n",
    "        self.lstm1 = nn.LSTM(input_size=conv_out_dim, hidden_size=hidden_dim,\n",
    "                             batch_first=True, bidirectional=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_dim * 2, hidden_size=hidden_dim,\n",
    "                             batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        sequence_output = outputs.last_hidden_state  # shape: [batch, seq_len, 768]\n",
    "\n",
    "        # Conv1D expects: [batch, channels, seq_len]\n",
    "        x = sequence_output.permute(0, 2, 1)  # → [batch, 768, seq_len]\n",
    "        x = self.conv1d(x)                   # → [batch, conv_out_dim, seq_len]\n",
    "        x = self.relu_conv(x)\n",
    "        x = x.permute(0, 2, 1)               # → [batch, seq_len, conv_out_dim]\n",
    "\n",
    "        # LSTM layers\n",
    "        lstm_out1, _ = self.lstm1(x)\n",
    "        relu_out = self.relu(lstm_out1)\n",
    "        lstm_out2, _ = self.lstm2(relu_out)\n",
    "\n",
    "        cls_token_out = lstm_out2[:, 0, :]  # Get the [CLS] token output\n",
    "\n",
    "        logits = self.classifier(cls_token_out)\n",
    "        return logits.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e03c78ab-b742-4a6a-a3c7-188c60c0bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pos = (df_whole['label'] == 1).sum()\n",
    "num_neg = (df_whole['label'] == 0).sum()\n",
    "# Calculate the ratio of class imbalance\n",
    "pos_weight = torch.tensor([num_neg / num_pos]).to(device)\n",
    "\n",
    "# Define the weighted loss function\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30a6ac8c-74f5-4bfe-805c-e312e90ba0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CamembertCNNLSTMClassifier().to(device)\n",
    "\n",
    "# Freeze all CamemBERT layers\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze ONLY the last transformer layer (layer 11, 0-indexed)\n",
    "for param in model.backbone.encoder.layer[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=1e-4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a22f6cb3-c20f-4894-8188-e6542843d261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [01:43<41:34, 103.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 — Loss: 0.9447 | Accuracy: 0.6944 | F1: 0.5594 | AUC: 0.7584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [03:29<40:13, 104.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25 — Loss: 0.8204 | Accuracy: 0.7328 | F1: 0.6453 | AUC: 0.8122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [05:15<38:41, 105.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25 — Loss: 0.7204 | Accuracy: 0.7399 | F1: 0.6642 | AUC: 0.8290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4/25 [07:02<37:08, 106.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25 — Loss: 0.6872 | Accuracy: 0.7697 | F1: 0.6817 | AUC: 0.8368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [08:49<35:29, 106.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25 — Loss: 0.6691 | Accuracy: 0.7719 | F1: 0.6453 | AUC: 0.8416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [10:39<34:03, 107.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25 — Loss: 0.6607 | Accuracy: 0.7598 | F1: 0.6859 | AUC: 0.8423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [12:28<32:22, 107.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25 — Loss: 0.6408 | Accuracy: 0.7434 | F1: 0.6802 | AUC: 0.8451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [14:15<30:28, 107.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 — Loss: 0.6239 | Accuracy: 0.7484 | F1: 0.6856 | AUC: 0.8453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9/25 [16:00<28:32, 107.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25 — Loss: 0.6213 | Accuracy: 0.7733 | F1: 0.6894 | AUC: 0.8487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10/25 [17:46<26:38, 106.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 — Loss: 0.6119 | Accuracy: 0.7598 | F1: 0.6927 | AUC: 0.8506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [19:32<24:47, 106.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 — Loss: 0.6165 | Accuracy: 0.7918 | F1: 0.6880 | AUC: 0.8512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12/25 [21:17<22:57, 105.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25 — Loss: 0.5878 | Accuracy: 0.7832 | F1: 0.6935 | AUC: 0.8538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [23:02<21:08, 105.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25 — Loss: 0.5873 | Accuracy: 0.7761 | F1: 0.6974 | AUC: 0.8540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14/25 [24:47<19:20, 105.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25 — Loss: 0.5919 | Accuracy: 0.7669 | F1: 0.6882 | AUC: 0.8537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15/25 [26:32<17:33, 105.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25 — Loss: 0.5797 | Accuracy: 0.7946 | F1: 0.6869 | AUC: 0.8566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [28:17<15:47, 105.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 — Loss: 0.5682 | Accuracy: 0.7790 | F1: 0.6918 | AUC: 0.8566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17/25 [30:02<14:01, 105.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25 — Loss: 0.6025 | Accuracy: 0.7846 | F1: 0.7015 | AUC: 0.8548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18/25 [31:47<12:15, 105.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25 — Loss: 0.5579 | Accuracy: 0.7854 | F1: 0.7085 | AUC: 0.8598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19/25 [33:32<10:30, 105.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25 — Loss: 0.5527 | Accuracy: 0.7925 | F1: 0.6812 | AUC: 0.8572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20/25 [35:17<08:45, 105.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25 — Loss: 0.5453 | Accuracy: 0.7910 | F1: 0.7072 | AUC: 0.8626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21/25 [37:02<07:00, 105.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25 — Loss: 0.5388 | Accuracy: 0.7910 | F1: 0.7024 | AUC: 0.8632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22/25 [38:47<05:15, 105.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25 — Loss: 0.5329 | Accuracy: 0.7960 | F1: 0.6850 | AUC: 0.8600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23/25 [40:32<03:30, 105.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25 — Loss: 0.5205 | Accuracy: 0.7882 | F1: 0.6996 | AUC: 0.8598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24/25 [42:17<01:45, 105.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25 — Loss: 0.5273 | Accuracy: 0.7441 | F1: 0.6907 | AUC: 0.8586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [44:02<00:00, 105.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25 — Loss: 0.5200 | Accuracy: 0.7918 | F1: 0.6989 | AUC: 0.8580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask).squeeze()\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask).squeeze()\n",
    "            probs = torch.sigmoid(logits)\n",
    "\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_preds.extend((probs > 0.5).int().cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} — Loss: {total_loss/len(train_loader):.4f} | \"\n",
    "          f\"Accuracy: {accuracy:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "462f840e-a702-4c7f-bda3-99d7bbed209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"camembert_cnn_lstm_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57ce25fd-33d4-4a2d-948f-18c6f6447700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(text, model, tokenizer, device, threshold=0.5):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "    prob = torch.sigmoid(logits).item()\n",
    "    prediction = 1 if prob >= threshold else 0\n",
    "    return prediction, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ea1d600-9ac7-42a3-bf5a-2fbfc76198c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 1 (probability: 0.7639)\n"
     ]
    }
   ],
   "source": [
    "text = \"la discrimination contre les femmes est un problème\"\n",
    "label, probability = predict_label(text, model, tokenizer, device)\n",
    "\n",
    "print(f\"Predicted label: {label} (probability: {probability:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e1b61bb1-086e-4415-9fcf-df8bbcc8e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can notice that even if we tried to avoid false positives, there is still a slight problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491c3ed3-58b7-48fb-ad0a-94f36e0601ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
