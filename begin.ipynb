{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "452ad336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import playwright\n",
    "import jmespath\n",
    "from playwright.sync_api import sync_playwright\n",
    "from twikit import Client\n",
    "import json\n",
    "import transformers\n",
    "import sentencepiece\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ab13aa",
   "metadata": {},
   "source": [
    "The command to use in order to have access to a tweet with its id is :\n",
    "\n",
    "``twitter.com/anyuser/status/541278904204668929\n",
    "``\n",
    "With 541278904204668929 being an example of a tweet's id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbacd70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/patriChiril/An-Annotated-Corpus-for-Sexism-Detection-in-French-Tweets/refs/heads/master/corpus_SexistContent.csv'\n",
    "df_base = pd.read_csv(url, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "207a22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.columns = ['id','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69825deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.to_csv('data_id.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "786cc0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://docs.google.com/spreadsheets/d/1eoC3NooDOwFoOvfweVLEZ-z-jCPZghUCVMUPTBkUDIk/export?format=csv&gid=1834088666'\n",
    "df_whole = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6626f46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>330425562201993216</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>326796299179548672</td>\n",
       "      <td>@MamzelleMNa Une très humble femme! #Ironie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>334548844731826176</td>\n",
       "      <td>BLOGUE - «Tsé, la parité homme-femme...» au se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332500824150392833</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>334424362033762304</td>\n",
       "      <td>Je suis une femme matérialiste et superficiell...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                               text\n",
       "0  330425562201993216                                                NaN\n",
       "1  326796299179548672        @MamzelleMNa Une très humble femme! #Ironie\n",
       "2  334548844731826176  BLOGUE - «Tsé, la parité homme-femme...» au se...\n",
       "3  332500824150392833                                                NaN\n",
       "4  334424362033762304  Je suis une femme matérialiste et superficiell..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_whole.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0dd4e8",
   "metadata": {},
   "source": [
    "### Model's import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "449faf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertModel, CamembertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c25fc598",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = 'tarte au prout et au caca'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5627f3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertModel were not initialized from the model checkpoint at camembert/camembert-base-wikipedia-4gb and are newly initialized: ['embeddings.word_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CamembertModel(\n",
       "  (embeddings): CamembertEmbeddings(\n",
       "    (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): CamembertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x CamembertLayer(\n",
       "        (attention): CamembertAttention(\n",
       "          (self): CamembertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): CamembertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): CamembertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): CamembertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): CamembertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import CamembertModel, CamembertTokenizer\n",
    "\n",
    "# You can replace \"camembert-base\" with any other model from the table, e.g. \"camembert/camembert-large\".\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert/camembert-base-wikipedia-4gb\")\n",
    "camembert = CamembertModel.from_pretrained(\"camembert/camembert-base-wikipedia-4gb\")\n",
    "\n",
    "camembert.eval()  # disable dropout (or leave in train mode to finetune)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea6f7ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = tokenizer.tokenize('tarte au caca')\n",
    "encoded_sentence = tokenizer.encode(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52dd07b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sentence = torch.tensor(encoded_sentence).unsqueeze(0)\n",
    "embeddings, _ = camembert(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b380e25",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "embeddings.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdf3ba85-b88f-4b6b-8c98-b88f53a4e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sentence = tokenizer.encode(tokenized_sentence)\n",
    "# [5, 221, 10, 10600, 14, 8952, 10540, 75, 1114, 6]\n",
    "# NB: Can be done in one step : tokenize.encode(\"J'aime le camembert !\")\n",
    "\n",
    "# Feed tokens to Camembert as a torch tensor (batch dim 1)\n",
    "encoded_sentence = torch.tensor(encoded_sentence).unsqueeze(0)\n",
    "embeddings, _ = camembert(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af27e977-9d39-43a8-baeb-fc2a0e270bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'last_hidden_state'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69bb46d6-00fb-409b-8ee5-3a252b183500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Tokenize in sub-words with SentencePiece\n",
    "tokenized_sentence = tokenizer.tokenize(\"J'aime le camembert !\")\n",
    "# ['▁J', \"'\", 'aime', '▁le', '▁ca', 'member', 't', '▁!'] \n",
    "\n",
    "# 1-hot encode and add special starting and end tokens \n",
    "encoded_sentence = tokenizer.encode(tokenized_sentence)\n",
    "# [5, 221, 10, 10600, 14, 8952, 10540, 75, 1114, 6]\n",
    "# NB: Can be done in one step : tokenize.encode(\"J'aime le camembert !\")\n",
    "\n",
    "# Feed tokens to Camembert as a torch tensor (batch dim 1)\n",
    "encoded_sentence = torch.tensor(encoded_sentence).unsqueeze(0)\n",
    "embeddings, _ = camembert(encoded_sentence)\n",
    "# embeddings.detach()\n",
    "# embeddings.size torch.Size([1, 10, 768])\n",
    "#tensor([[[-0.0928,  0.0506, -0.0094,  ..., -0.2388,  0.1177, -0.1302],\n",
    "#         [ 0.0662,  0.1030, -0.2355,  ..., -0.4224, -0.0574, -0.2802],\n",
    "#         [-0.0729,  0.0547,  0.0192,  ..., -0.1743,  0.0998, -0.2677],\n",
    "#         ...,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8e91e85-6d5f-44a6-934d-20334a7fa051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'last_hidden_state'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec11ebee-ab74-4364-8452-6859c9ec2756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertModel were not initialized from the model checkpoint at camembert/camembert-base-wikipedia-4gb and are newly initialized: ['embeddings.word_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CamembertModel(\n",
       "  (embeddings): CamembertEmbeddings(\n",
       "    (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): CamembertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x CamembertLayer(\n",
       "        (attention): CamembertAttention(\n",
       "          (self): CamembertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): CamembertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): CamembertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): CamembertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): CamembertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import CamembertModel, CamembertTokenizer\n",
    "\n",
    "# You can replace \"camembert-base\" with any other model from the table, e.g. \"camembert/camembert-large\".\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert/camembert-base-wikipedia-4gb\")\n",
    "camembert = CamembertModel.from_pretrained(\"camembert/camembert-base-wikipedia-4gb\")\n",
    "\n",
    "camembert.eval()  # disable dropout (or leave in train mode to finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2d00b3cd-3b1e-4a34-a5bd-5de76aa855a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Tokenize in sub-words with SentencePiece\n",
    "tokenized_sentence = tokenizer.tokenize(\"J'aime le camembert !\")\n",
    "# ['▁J', \"'\", 'aime', '▁le', '▁ca', 'member', 't', '▁!'] \n",
    "\n",
    "# 1-hot encode and add special starting and end tokens \n",
    "encoded_sentence = tokenizer.encode(tokenized_sentence)\n",
    "# [5, 221, 10, 10600, 14, 8952, 10540, 75, 1114, 6]\n",
    "# NB: Can be done in one step : tokenize.encode(\"J'aime le camembert !\")\n",
    "\n",
    "# Feed tokens to Camembert as a torch tensor (batch dim 1)\n",
    "encoded_sentence = torch.tensor(encoded_sentence).unsqueeze(0)\n",
    "embeddings, _ = camembert(encoded_sentence)\n",
    "# embeddings.detach()\n",
    "# embeddings.size torch.Size([1, 10, 768])\n",
    "#tensor([[[-0.0928,  0.0506, -0.0094,  ..., -0.2388,  0.1177, -0.1302],\n",
    "#         [ 0.0662,  0.1030, -0.2355,  ..., -0.4224, -0.0574, -0.2802],\n",
    "#         [-0.0729,  0.0547,  0.0192,  ..., -0.1743,  0.0998, -0.2677],\n",
    "#         ...,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e255ebf-2479-427f-b6d4-fe53ee440cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'last_hidden_state'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e852159e-3838-42fe-ae7f-22f8158c208e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    5,   221,    10, 10600,    14,  8952, 10540,    75,  1114,     6]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70507812-62cc-414f-8df8-bb7a95b1150c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: <s>\n",
      "Embedding: [-0.025433994829654694, 0.023452727124094963, 0.1026916578412056, -0.07934340834617615, -0.040274087339639664]...\n",
      "Token: ▁J\n",
      "Embedding: [0.060596346855163574, -0.18112367391586304, -0.041815709322690964, 0.10336349159479141, -0.17535389959812164]...\n",
      "Token: '\n",
      "Embedding: [-0.15609169006347656, -0.11271191388368607, 0.2687494158744812, 0.14070138335227966, -0.10274503380060196]...\n",
      "Token: aime\n",
      "Embedding: [0.10450512915849686, -0.38123050332069397, -0.08222424983978271, -0.051813967525959015, 0.014235161244869232]...\n",
      "Token: ▁le\n",
      "Embedding: [-0.03520742431282997, -0.09182063490152359, -0.018093395978212357, -0.057230144739151, -0.05666176229715347]...\n",
      "Token: ▁ca\n",
      "Embedding: [-0.00826512835919857, -0.06211479380726814, 0.07769035547971725, 0.04952171444892883, -0.051639020442962646]...\n",
      "Token: member\n",
      "Embedding: [0.004028766416013241, -0.25367453694343567, 0.26399505138397217, 0.003843585727736354, 0.047274839133024216]...\n",
      "Token: t\n",
      "Embedding: [0.059481263160705566, -0.09754593670368195, -0.014385445974767208, -0.0030507552437484264, 0.058047614991664886]...\n",
      "Token: ▁!\n",
      "Embedding: [0.12447205185890198, 0.19101697206497192, -0.46814337372779846, 0.09177287667989731, 0.024526068940758705]...\n",
      "Token: </s>\n",
      "Embedding: [-0.08578962832689285, 0.07651311159133911, 0.11205431073904037, -0.10313579440116882, -0.08548953384160995]...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "\n",
    "# Load CamemBERT model and tokenizer\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertModel.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Input sentence\n",
    "sentence = \"J'aime le camembert !\"\n",
    "\n",
    "# Tokenize and convert to PyTorch tensors\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", return_attention_mask=True)\n",
    "\n",
    "# Forward pass (no gradients needed)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get token embeddings from last hidden state: [1, seq_len, 768]\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "# Decode tokens to match embeddings with actual text tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "# Print token-wise embeddings\n",
    "for idx, token in enumerate(tokens):\n",
    "    embedding = last_hidden_state[0, idx]  # shape: [768]\n",
    "    print(f\"Token: {token}\\nEmbedding: {embedding.tolist()[:5]}...\")  # first 5 dims for brevity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e631afd1-9152-4c7b-96d5-c88c76bc0499",
   "metadata": {},
   "source": [
    "Exemple de bite en bois de merde : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b9570244-9830-4815-96f9-f5a0e5136232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Sentence 1:\n",
      "  Token: <s>          Embedding: [-0.025434013456106186, 0.02345276065170765, 0.10269155353307724]...\n",
      "  Token: ▁J           Embedding: [0.06059642136096954, -0.18112397193908691, -0.041815921664237976]...\n",
      "  Token: '            Embedding: [-0.15609155595302582, -0.11271196603775024, 0.2687494158744812]...\n",
      "  Token: aime         Embedding: [0.1045050248503685, -0.38123074173927307, -0.08222481608390808]...\n",
      "  Token: ▁le          Embedding: [-0.03520761430263519, -0.09182073175907135, -0.018093368038535118]...\n",
      "  Token: ▁ca          Embedding: [-0.008264961652457714, -0.06211426854133606, 0.07768992334604263]...\n",
      "  Token: member       Embedding: [0.00402885302901268, -0.25367507338523865, 0.26399531960487366]...\n",
      "  Token: t            Embedding: [0.05948113277554512, -0.09754568338394165, -0.014385613612830639]...\n",
      "  Token: ▁!           Embedding: [0.12447208911180496, 0.19101713597774506, -0.468143492937088]...\n",
      "  Token: </s>         Embedding: [-0.08578969538211823, 0.07651315629482269, 0.11205412447452545]...\n",
      "\n",
      "Sentence 2:\n",
      "  Token: <s>          Embedding: [-0.025243820622563362, 0.04026903212070465, 0.1326451301574707]...\n",
      "  Token: ▁Les         Embedding: [-0.01049117837101221, -0.10779692232608795, 0.13669292628765106]...\n",
      "  Token: ▁chats       Embedding: [0.04276338964700699, 0.16595083475112915, 0.2856592833995819]...\n",
      "  Token: ▁dorment     Embedding: [-0.06167614459991455, -0.06424520909786224, 0.1929420828819275]...\n",
      "  Token: ▁sur         Embedding: [0.0003479187726043165, -0.0675036758184433, -0.09761910140514374]...\n",
      "  Token: ▁le          Embedding: [-0.12068771570920944, -0.04440688341856003, 0.03007631190121174]...\n",
      "  Token: ▁canapé      Embedding: [0.001076913671568036, -0.15123824775218964, -0.05446786433458328]...\n",
      "  Token: .            Embedding: [-0.09951363503932953, 0.02012793719768524, 0.17151275277137756]...\n",
      "  Token: </s>         Embedding: [-0.13268254697322845, 0.12828904390335083, 0.14001120626926422]...\n",
      "  Token: <pad>        Embedding: [-0.13318273425102234, 0.129573255777359, 0.1391066014766693]...\n",
      "\n",
      "Sentence 3:\n",
      "  Token: <s>          Embedding: [-0.05122503265738487, 0.037091001868247986, 0.12693631649017334]...\n",
      "  Token: ▁Il          Embedding: [-0.02366972155869007, -0.1694360375404358, -0.16274867951869965]...\n",
      "  Token: ▁pleut       Embedding: [-0.03051239438354969, -0.2957800626754761, -0.2616053521633148]...\n",
      "  Token: ▁souvent     Embedding: [0.04842151328921318, 0.1877518743276596, -0.2613389194011688]...\n",
      "  Token: ▁à           Embedding: [0.09452281892299652, 0.12983684241771698, 0.059222232550382614]...\n",
      "  Token: ▁Paris       Embedding: [0.008266408927738667, 0.18258778750896454, -0.1393500566482544]...\n",
      "  Token: .            Embedding: [-0.1233210638165474, 0.00908589456230402, 0.1654377430677414]...\n",
      "  Token: </s>         Embedding: [-0.13499097526073456, 0.10215505957603455, 0.1280384510755539]...\n",
      "  Token: <pad>        Embedding: [-0.13792936503887177, 0.11036666482686996, 0.12433220446109772]...\n",
      "  Token: <pad>        Embedding: [-0.13792936503887177, 0.11036666482686996, 0.12433220446109772]...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import CamembertTokenizerFast, CamembertModel\n",
    "tokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\")\n",
    "\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\")\n",
    "model = CamembertModel.from_pretrained(\"camembert-base\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text': [\n",
    "        \"J'aime le camembert !\",\n",
    "        \"Les chats dorment sur le canapé.\",\n",
    "        \"Il pleut souvent à Paris.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Tokenize and batch the text\n",
    "inputs = tokenizer(\n",
    "    df['text'].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    return_attention_mask=True,\n",
    "    return_offsets_mapping=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "# Move inputs to GPU\n",
    "input_ids = inputs['input_ids'].to(device)\n",
    "attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "# Run model on GPU\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Get token embeddings (last hidden state)\n",
    "token_embeddings = outputs.last_hidden_state  # shape: [batch_size, seq_len, 768]\n",
    "\n",
    "# Optionally move embeddings back to CPU\n",
    "token_embeddings_cpu = token_embeddings.cpu()\n",
    "\n",
    "# Convert input IDs to tokens\n",
    "batch_tokens = [\n",
    "    tokenizer.convert_ids_to_tokens(seq_ids) for seq_ids in input_ids.cpu()\n",
    "]\n",
    "\n",
    "# Display tokens and their embeddings (first 3 dimensions shown for brevity)\n",
    "for i, tokens in enumerate(batch_tokens):\n",
    "    print(f\"\\nSentence {i + 1}:\")\n",
    "    for j, token in enumerate(tokens):\n",
    "        vector = token_embeddings_cpu[i, j]\n",
    "        print(f\"  Token: {token.ljust(12)} Embedding: {vector[:3].tolist()}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a6aada-7bd8-48ff-b6cd-192f5281baf5",
   "metadata": {},
   "source": [
    "Exemple de retrain a la con de merde : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c6d1b380-7959-402e-87bc-9b99a7c9a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CamembertModel\n",
    "\n",
    "class CamembertSentenceClassifier(nn.Module):\n",
    "    def __init__(self, hidden_dim=768, mid_dim=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load pre-trained CamemBERT\n",
    "        self.bert = CamembertModel.from_pretrained(\"camembert-base\")\n",
    "        \n",
    "        # Freeze all BERT parameters\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Classification head (only this is trainable)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, mid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mid_dim, 1),      # Binary classification\n",
    "            nn.Sigmoid()                # Output in [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use mean pooling over token embeddings to get sentence representation\n",
    "        token_embeddings = outputs.last_hidden_state       # [batch_size, seq_len, 768]\n",
    "        pooled = token_embeddings.mean(dim=1)              # [batch_size, 768]\n",
    "\n",
    "        # Classifier outputs probability\n",
    "        return self.classifier(pooled)                     # [batch_size, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff096aa1-9d1f-4049-acf5-2924234c73f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = CamembertSentenceClassifier()\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer (only train classifier parameters)\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=1e-4)\n",
    "\n",
    "# Loss function for binary classification\n",
    "loss_fn = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76c9c0b-ff40-4b02-874d-864702060889",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device).float()  # or .long() for multi-class\n",
    "\n",
    "    outputs = model(input_ids, attention_mask)\n",
    "\n",
    "    # Use appropriate loss\n",
    "    if model.is_binary:\n",
    "        loss_fn = nn.BCELoss()\n",
    "    else:\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    loss = loss_fn(outputs.squeeze(), labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5afcb84-8238-4e8c-853c-897f020fb999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import CamembertTokenizerFast, CamembertModel\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\")\n",
    "model = CamembertModel.from_pretrained(\"camembert-base\").to(device)\n",
    "model.eval()\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text': [\n",
    "        \"J'aime le camembert !\",\n",
    "        \"Les chats dorment sur le canapé.\",\n",
    "        \"Il pleut souvent à Paris.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Tokenize and encode all sentences at once\n",
    "inputs = tokenizer(\n",
    "    df['text'].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "# Move tensors to GPU\n",
    "input_ids = inputs['input_ids'].to(device)\n",
    "attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "# Generate embeddings (with no gradients)\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Extract sentence embeddings by averaging token embeddings (mean pooling)\n",
    "token_embeddings = outputs.last_hidden_state  # [batch_size, seq_len, hidden]\n",
    "sentence_embeddings = token_embeddings.mean(dim=1)  # [batch_size, hidden_dim]\n",
    "\n",
    "# Convert to CPU for further use\n",
    "sentence_embeddings_cpu = sentence_embeddings.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6621ce-3541-4621-aeac-e884496f7d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels (for demonstration, you'll need to replace this with your own labels)\n",
    "df['label'] = [1, 0, 1]  # Example labels, modify accordingly\n",
    "\n",
    "# Features (sentence embeddings) and Labels\n",
    "X = sentence_embeddings_cpu  # Embeddings\n",
    "y = df['label'].values       # Labels (0 or 1)\n",
    "\n",
    "# Convert to tensors for PyTorch\n",
    "X_tensor = torch.tensor(X).float().to(device)\n",
    "y_tensor = torch.tensor(y).float().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3991004d-3c66-4e8f-b721-cc0e44411d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define your classifier (same as before)\n",
    "class CamembertSentenceClassifier(nn.Module):\n",
    "    def __init__(self, hidden_dim=768, mid_dim=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, mid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mid_dim, 1),      # Binary classification\n",
    "            nn.Sigmoid()                # Output in [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)  # [batch_size, 1]\n",
    "\n",
    "# Initialize the model\n",
    "model = CamembertSentenceClassifier().to(device)\n",
    "\n",
    "# Loss function for binary classification\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# Optimizer (only for the classifier layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(X_tensor)  # Shape: [batch_size, 1]\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_fn(outputs.squeeze(), y_tensor)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
